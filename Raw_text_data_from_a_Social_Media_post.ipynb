{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvc1TNpff8CyaxxBfd5YzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarathi-vs13/Natural-Language-Processing/blob/main/Raw_text_data_from_a_Social_Media_post.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting raw text data from a messy social media post"
      ],
      "metadata": {
        "id": "psfRWPU-Xmtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using NLTK"
      ],
      "metadata": {
        "id": "cJIALLn7k5C0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nepHWY1qWf9D"
      },
      "outputs": [],
      "source": [
        "#nltk.download('popular')\n",
        "#nltk.download('punkt_tab')\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  clean_tokens = []\n",
        "  for token in tokens:\n",
        "    if token.startswith(\"@\") or token.startswith(\".\") or token.startswith(\"#\"):\n",
        "      continue\n",
        "    if token.startswith(\"http\") or token.startswith(\"www\"):\n",
        "      continue\n",
        "    if token in string.punctuation:\n",
        "      continue\n",
        "    if token.isdigit():\n",
        "      continue\n",
        "    clean_tokens.append(token)\n",
        "\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered = [ word for word in clean_tokens if word not in stop_words]\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in filtered]\n",
        "\n",
        "  return lemmatized\n"
      ],
      "metadata": {
        "id": "BGQkqHKHYOtW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messy_text = \"\"\"\n",
        "5 min ago my reddit all turned spanish....all the tabs, preferences..etc. I went into preferences and made sure they were checked to english....they were....what is going on? I cant read spanish so I am in need of some help here....i am asking you b/c I cannot find the mod help link b/c I cannot read it.\n",
        "\n",
        "UPDATE- ok- it must have something to do with firefox. And to all of you telling me how to change lang. preference, OF COURSE I TRIED THAT before I posted. On IE all is normal. On my desktop all is normal. On my netbook, using firefox, it is a taco show. I ran the page through google translator and I especially enjoy the rick roll. So anyone know how to un-spanish reddit in firefox? This is the only page it is happening on.\n",
        "\n",
        "EDIT- I must admit this is hilarious. I wish i had paid more attention in spanish class....\n",
        "\n",
        "UPDATE- So I wake up this morning to about 1500 replies in my inbox that I cannot read. And then I run them through Google translator and most of them say stuff like \"the dog is in my pants\" and \"where is the library\".\n",
        "\n",
        "Thanks, reddit.\n",
        "\n",
        "As far as the Spanish problem goes.. I disabled all my firefox extensions, cleared all my cookies and restarted it all again. THE SPANISH IS GONE! I do not know what possessed my computer to run for the border, but I am glad it is back. :)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(clean_text(messy_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJie3YyZdj7E",
        "outputId": "dee74efd-8a3e-4819-8a87-c7b59f1dab85"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['min', 'ago', 'reddit', 'turn', 'spanish', 'tabs', 'preferences', 'etc', 'go', 'preferences', 'make', 'sure', 'check', 'english', 'go', 'cant', 'read', 'spanish', 'need', 'help', 'ask', 'b/c', 'find', 'mod', 'help', 'link', 'b/c', 'read', 'update-', 'ok-', 'must', 'something', 'firefox', 'tell', 'change', 'lang', 'preference', 'course', 'try', 'post', 'ie', 'normal', 'desktop', 'normal', 'netbook', 'use', 'firefox', 'taco', 'show', 'run', 'page', 'google', 'translator', 'especially', 'enjoy', 'rick', 'roll', 'anyone', 'know', 'un-spanish', 'reddit', 'firefox', 'page', 'happen', 'edit-', 'must', 'admit', 'hilarious', 'wish', 'pay', 'attention', 'spanish', 'class', 'update-', 'wake', 'morning', 'reply', 'inbox', 'read', 'run', 'google', 'translator', 'say', 'stuff', 'like', '``', 'dog', 'pant', \"''\", '``', 'library', \"''\", 'thank', 'reddit', 'far', 'spanish', 'problem', 'go', 'disable', 'firefox', 'extensions', 'clear', 'cookies', 'restart', 'spanish', 'go', 'know', 'possess', 'computer', 'run', 'border', 'glad', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing a function that return the most frequent lemmas used in the above social media post."
      ],
      "metadata": {
        "id": "DUoxoY80nqeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_freq_lemmas(lemmatized_list, n =5):\n",
        "  freq = Counter(lemmatized_list)\n",
        "  return freq.most_common(n)"
      ],
      "metadata": {
        "id": "W54cIvhHetp5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = clean_text(messy_text)\n",
        "top_lemmas = most_freq_lemmas(lemmas)\n",
        "\n",
        "print(top_lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxl7nA6rhCnx",
        "outputId": "e78a38a7-8bd9-4500-d937-7d67dccf7b26"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spanish', 5), ('go', 4), ('firefox', 4), ('reddit', 3), ('read', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using spaCy"
      ],
      "metadata": {
        "id": "prCO9S0ek94n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_text_spacy(text):\n",
        "  doc = nlp(text)\n",
        "  clean_tokens = []\n",
        "  for token in doc:\n",
        "    if (\n",
        "        not token.is_stop\n",
        "        and not token.is_punct\n",
        "        and token.is_alpha\n",
        "    ):\n",
        "      clean_tokens.append(token.lemma_.lower())\n",
        "  return clean_tokens"
      ],
      "metadata": {
        "id": "QzBOXlEYknVZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "EA's infamous \"pride and accomplishment\" post that got them 668,000 downvotes, the most downvoted post in the history of Reddit. Funnily enough the post is also one of the highest awarded in the site, presumably because awards help bring visibility to the post which will make more people downvote it.\n",
        "\n",
        "\"\"\"\n",
        "print(clean_text_spacy(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DftXHo6QmIuO",
        "outputId": "16529005-d921-4bd7-e84f-3456623cd533"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ea', 'infamous', 'pride', 'accomplishment', 'post', 'get', 'downvote', 'downvoted', 'post', 'history', 'reddit', 'funnily', 'post', 'high', 'award', 'site', 'presumably', 'award', 'help', 'bring', 'visibility', 'post', 'people', 'downvote']\n"
          ]
        }
      ]
    }
  ]
}