{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCcrcPdKbCEq268SmK0jlT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarathi-vs13/Natural-Language-Processing/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "8Y_VgqrZU_QO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "fNwV8uT4BCTM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "collapsed": true,
        "id": "sOyFRoN6-9oL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('popular')\n",
        "#nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(\"I'm learning NLP\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLze8HMn_feD",
        "outputId": "6c64cd8b-06c7-45cd-c271-8053569a152c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'learning', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "B9bjlX0KB6E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"ran\", \"swimming\", \"playing\", \"plays\", \"happily\", \"lazy\"]\n",
        "\n",
        "for word in words:\n",
        "  print(word, \"→\", stemmer.stem(word) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1u-7GWBB8Pq",
        "outputId": "be269864-3643-4959-d515-3fef3e985b7e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running → run\n",
            "ran → ran\n",
            "swimming → swim\n",
            "playing → play\n",
            "plays → play\n",
            "happily → happili\n",
            "lazy → lazi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "fWRcciQSDyT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"dancing\", \"happily\", \"lazy\", \"written\",\"goes\"]\n",
        "\n",
        "for word in words:\n",
        "  print(word, \"→\", lemmatizer.lemmatize(word, pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844GuLtxDZHA",
        "outputId": "8dde3601-2a66-4a0c-8214-f1f82277f4d3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running → run\n",
            "dancing → dance\n",
            "happily → happily\n",
            "lazy → lazy\n",
            "written → write\n",
            "goes → go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using spaCy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The weather in chennai is unusually romantic today.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, \"→\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbuTfTSsJmdG",
        "outputId": "a4bb8894-7108-4a0f-81c1-46068e5bb2af"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → the\n",
            "weather → weather\n",
            "in → in\n",
            "chennai → chennai\n",
            "is → be\n",
            "unusually → unusually\n",
            "romantic → romantic\n",
            "today → today\n",
            ". → .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword"
      ],
      "metadata": {
        "id": "ZlyNxWqtGrRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Let's see an example of a simple stopword removal code by using spacy\")\n",
        "\n",
        "filtered = []\n",
        "\n",
        "for token in doc:\n",
        "  if not token.is_stop:\n",
        "    filtered.append(token)\n",
        "\n",
        "print(filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsZSp_b9GLqE",
        "outputId": "a1397d8d-ee79-4ace-de4c-051a82e26cbf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Let, example, simple, stopword, removal, code, spacy]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "sCgMmKCGM4F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ0Gw4mMMf-P",
        "outputId": "eec36ad0-7c6e-44ff-8042-d6db37887a45"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using spaCy\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
        "for token in doc:\n",
        "  print(f\" {token.text} → {token.pos} ({token.tag_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQm-BYtvNvp5",
        "outputId": "8b7cd6b0-ecad-4943-88a6-e2876b1736dd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The → 90 (DT)\n",
            " quick → 84 (JJ)\n",
            " brown → 84 (JJ)\n",
            " fox → 92 (NN)\n",
            " jumps → 100 (VBZ)\n",
            " over → 85 (IN)\n",
            " the → 90 (DT)\n",
            " lazy → 84 (JJ)\n",
            " dog → 92 (NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "eyldCYkVRTfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Tesla was founded by Elon Musk in California in 2003.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"→\", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-L6du4yPAlu",
        "outputId": "02f76066-5594-4722-fcef-fd2ba0ca1fa7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk → PERSON\n",
            "California → GPE\n",
            "2003 → DATE\n"
          ]
        }
      ]
    }
  ]
}